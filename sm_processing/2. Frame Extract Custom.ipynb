{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing for Video Frame Extraction (Container)\n",
    "\n",
    "In this follow-on notebook, we'll tackle the same problem with a custom container image to improve job execution speed and cost.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Pre-requisites\n",
    "\n",
    "This notebook will create an ECR repository and push an image, so you'll need to **grant the notebook instance permissions permissions to use ECR** if running in SageMaker.\n",
    "\n",
    "In simple steps to get started (with a permissive configuration that production users may want to limit down further):\n",
    "\n",
    "- Go to the \"Notebook instances\" tab of the SageMaker console\n",
    "- Find this notebook instance in the list, and click on the hyperlinked notebook name to go to the details page\n",
    "- Scroll down to the \"Permissions and encryption\" section, and click on the hyperlinked \"IAM role ARN\" - which will open the IAM role details screen in a new tab.\n",
    "- Click the blue \"Attach Policies\" button and search for the `AmazonEC2ContainerRegistryPowerUser` policy: Attach this policy to the role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dependencies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import os\n",
    "from string import Template\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Re-use previous notebook setup\n",
    "\n",
    "We downloaded the sample data and set configurations like the `BUCKET_NAME` in the previous notebook, so we won't repeat ourselves here! Just reload the config and init the libraries as before.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r BUCKET_NAME\n",
    "%store -r INPUT_PREFIX\n",
    "%store -r OUTPUT_PREFIX\n",
    "\n",
    "if not os.path.isdir(f\"{os.path.abspath('')}/{INPUT_PREFIX}\"):\n",
    "    raise RuntimeError(\"You need to run the non-ECR notebook's setup and data download first!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Connect to container registries\n",
    "\n",
    "We will inherit from the SageMaker SKLearn base container (so we need to know the URI where it lives, and log in to the repository); and create a new image (so we need to log in to our own registry to store it).\n",
    "\n",
    "Since the base [Processor](https://sagemaker.readthedocs.io/en/stable/processing.html) interface of the Python SageMaker SDK takes an `image_uri` parameter, and the standard `SKLearnProcessor` used in the previous notebook only needs the SciKit Learn framework version, we can infer the [(open source) implementation of SKLearnProcessor](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/sklearn/processing.py) in the SDK will show us how to derive the container URI.\n",
    "\n",
    "...so here we pretty much copy the approach taken by the SDK to derive the base image URI:\n",
    "\n",
    "(Which we need to do, because it varies with AWS Region and other factors!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "framework_version = \"0.20.0\"\n",
    "image_tag = \"{}-{}-{}\".format(framework_version, \"cpu\", \"py3\")\n",
    "image_uri = sagemaker.fw_registry.default_framework_uri(\"scikit-learn\", region, image_tag)\n",
    "# (Note, some other frameworks use the `sagemaker.fw_utils.create_image_uri()` function instead)\n",
    "\n",
    "base_host_account_id = image_uri.partition(\".\")[0]\n",
    "print(image_uri)\n",
    "print(f\"Base image host account: {base_host_account_id}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This host account is the first ECR registry we'll need to log in to (to pull the base image); and our own account is the other (to push our custom image). Here we open an ECR client and resolve our own account ID:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ECR is a separate service, so we'll need another service client:\n",
    "crclient = session.client(\"ecr\")\n",
    "# We also want our account ID for the purposes of logging in to our own ECR:\n",
    "account_id = session.client(\"sts\").get_caller_identity().get(\"Account\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we log in to the two registries.\n",
    "\n",
    "The `aws ecr get-login` command returns (along with some other text) an executable `docker login` command with temporary credentials generated by IAM for our current AWS session.\n",
    "\n",
    "The below just executes the AWS CLI command and then the Docker CLI command in the result:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_cmd = f\"$(aws ecr get-login --registry-ids {base_host_account_id} {account_id} --no-include-email | sed 's|https://||')\"\n",
    "!eval \"$login_cmd\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Build and upload the custom container image\n",
    "\n",
    "This source repository uses a template Dockerfile because the base image URI is dynamically calculated as above, so our first step is to resolve the variable to create a concrete Dockerfile:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"container/Dockerfile.tpl\", \"r\") as tplfile:\n",
    "    with open(\"container/Dockerfile\", \"w\") as dockerfile:\n",
    "        template = Template(tplfile.read())\n",
    "        dockerfile.write(\n",
    "            template.substitute({\n",
    "                \"BASE_IMAGE\": image_uri\n",
    "            }),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we simply run a standard Docker build:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "container_name = \"smskcv\"\n",
    "!docker rmi --force $container_name\n",
    "!docker build -t $container_name:latest ./container\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parent image and the tagged child image should now both be available here in our notebook instance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our image is built!\n",
    "\n",
    "We'll create an ECR repository with the AWS CLI, and then associate the repository to the image and push it using standard Docker CLI:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws ecr create-repository --repository-name $container_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/smskcv:latest\"\n",
    "!docker tag smskcv:latest $target_uri\n",
    "!docker push $target_uri\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Use our new container for a processing job!\n",
    "\n",
    "Now we have our custom container, we can use it in place of the Scikit-Learn built-in by substituting `ScriptProcessor` for `SKLearnProcessor`, and adding a couple of extra arguments.\n",
    "\n",
    "Otherwise, the process to create and apply the processor is essentially the same as before:\n",
    "\n",
    "**Note: You might want to delete the old `OUTPUT_PREFIX` folder from S3 to convince yourself that the below job re-populates it**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = ScriptProcessor(\n",
    "    role=role,\n",
    "    image_uri=target_uri,  # Need to tell SageMaker where to find the custom image\n",
    "    command=[\"python3\", \"-v\"],  # Because it's a custom image, need to specify the start command\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    volume_size_in_gb=5, # We don't need the whole default allocation for this small data set!\n",
    "    instance_count=2,  # We can parallelize the processing to boost overall speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This command will block while the job runs and output the logs:\n",
    "processor.run(\n",
    "    code=\"getframes.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f\"s3://{BUCKET_NAME}/{INPUT_PREFIX}\",\n",
    "            destination=\"/opt/ml/processing/input/videos\",\n",
    "            # By default, each input will be \"FullyReplicated\": copied in full to every instance.\n",
    "            # This is great for any common reference data, but to parallelize processing of the\n",
    "            # main dataset we can use \"ShardedByS3Key\" to split the data between instances instead:\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"frames\",\n",
    "            source=\"/opt/ml/processing/frames\",\n",
    "            destination=f\"s3://{BUCKET_NAME}/{OUTPUT_PREFIX}\",\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\"--frames-per-second\", \"0\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-Up:\n",
    "\n",
    "As before, be aware of:\n",
    "\n",
    "* This notebook instance\n",
    "* The S3 input and output locations where we've stored data, and\n",
    "* Any new processing metadata saved to the SageMaker default bucket for this region\n",
    "\n",
    "If you use this notebook instance for other things, you might also want to clean up locally cached Docker images to free disk space - as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker image prune -a -f\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
