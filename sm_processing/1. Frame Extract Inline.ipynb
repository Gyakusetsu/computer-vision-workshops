{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SageMaker Processing for Video Frame Extraction (Inline)\n",
    "\n",
    "This example shows how to use SageMaker Processing to extract frame images from video files in batch.\n",
    "\n",
    "Our extractor implementation requires open source library [OpenCV](https://opencv.org/), which is **not installed in the built-in SageMaker Scikit-Learn processing container**. We show two ways to solve this:\n",
    "\n",
    "1. (This notebook) Simply use **inline commands** at the top of our Python script to install OS-level dependencies and the Python OpenCV library each time a job starts\n",
    "2. (Next notebook) Create a **custom container image** using the SageMaker built-in as a base, pre-installing the dependencies\n",
    "\n",
    "The second option (custom container) reduces the run-time and therefore the cost of each Processing job; while the first (inline install) is simpler to get working and avoids introducing the Elastic Container Registry (ECR) service.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Dependencies and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that although OpenCV isn't included in the optimized container images, it actually is \n",
    "# present here in the standard notebook conda_python3 kernel!\n",
    "#\n",
    "# In general though, we should prefer doing heavy pre-processing work in jobs over notebooks\n",
    "# to best utilize resources (since the resources for jobs are active only while the job is running)\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "# Built-Ins:\n",
    "import io\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "# External Dependencies:\n",
    "import boto3\n",
    "import requests\n",
    "import sagemaker\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NAME = sagemaker.Session().default_bucket()  # (Or an existing bucket's name, if you prefer)\n",
    "%store BUCKET_NAME\n",
    "INPUT_PREFIX = \"videos\" # The folder in the bucket (and locally) where raw videos will live\n",
    "%store INPUT_PREFIX\n",
    "OUTPUT_PREFIX = \"frames\" # The base folder in the bucket where output frames will be written\n",
    "%store OUTPUT_PREFIX\n",
    "\n",
    "os.makedirs(INPUT_PREFIX, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "role = sagemaker.get_execution_role()\n",
    "session = boto3.session.Session()\n",
    "region = session.region_name\n",
    "bucket = session.resource(\"s3\").Bucket(BUCKET_NAME)\n",
    "\n",
    "bucket_region = \\\n",
    "    session.client(\"s3\").head_bucket(Bucket=BUCKET_NAME)[\"ResponseMetadata\"][\"HTTPHeaders\"][\"x-amz-bucket-region\"]\n",
    "assert (\n",
    "    bucket_region == region\n",
    "), f\"Your S3 bucket {BUCKET_NAME} and this notebook need to be in the same region.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Push our source data into S3\n",
    "\n",
    "We'll be using a small collection of CC-0/public domain videos as an example: But you can replace this with whatever you'd like to process.\n",
    "\n",
    "The end result must be that the `INPUT_PREFIX` folder of your `BUCKET_NAME` contains one or more video files of format supported by OpenCV VideoCapture. Nested folders are not supported by this sample code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "request = requests.get(\"https://archive.org/compress/pigeons_sp/formats=512KB%20MPEG4&file=/pigeons_sp.zip\")\n",
    "vidzip = zipfile.ZipFile(io.BytesIO(request.content))\n",
    "for fname in vidzip.namelist():\n",
    "    with open(f\"{INPUT_PREFIX}/{fname}\", \"wb\") as f:\n",
    "        f.write(vidzip.read(fname))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 sync $INPUT_PREFIX s3://$BUCKET_NAME/$INPUT_PREFIX\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Run a SageMaker Processing Job\n",
    "\n",
    "Here we use the SageMaker Processing built-in Scikit-Learn container to run our job. This saves the complexity and cost of setting up a custom container image in ECR, but means that our job needs to install OpenCV and its dependencies every time it runs - which will add to our job time and therefore the SageMaker compute costs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = SKLearnProcessor(\n",
    "    framework_version=\"0.20.0\",\n",
    "    role=role,\n",
    "    instance_type=\"ml.t3.medium\",\n",
    "    volume_size_in_gb=5,  # We don't need the whole default allocation for this small data set!\n",
    "    instance_count=2,  # We can parallelize the processing to boost overall speed\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This command will block while the job runs and output the logs:\n",
    "processor.run(\n",
    "    code=\"getframes.py\",\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=f\"s3://{BUCKET_NAME}/{INPUT_PREFIX}\",\n",
    "            destination=\"/opt/ml/processing/input/videos\",\n",
    "            # By default, each input will be \"FullyReplicated\": copied in full to every instance.\n",
    "            # This is great for any common reference data, but to parallelize processing of the\n",
    "            # main dataset we can use \"ShardedByS3Key\" to split the data between instances instead:\n",
    "            s3_data_distribution_type=\"ShardedByS3Key\",\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name=\"frames\",\n",
    "            source=\"/opt/ml/processing/frames\",\n",
    "            destination=f\"s3://{BUCKET_NAME}/{OUTPUT_PREFIX}\",\n",
    "        ),\n",
    "    ],\n",
    "    arguments=[\"--frames-per-second\", \"0\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Querying Job Status\n",
    "\n",
    "In case we run a job in non-blocking mode, or just want to review after the job is complete, we can fetch the status of jobs as below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing_job_description = processor.jobs[-1].describe()\n",
    "preprocessing_job_description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean-Up\n",
    "\n",
    "The Processing container is shut down by SageMaker as soon as the job completes, so you only need to be aware of the ongoing S3 data storage and this running notebook instance.\n",
    "\n",
    "Be aware though that our default configuration here will also push some job metadata to the SageMaker default bucket in this region (`sagemaker-{regionname}-{accountid}`): Why not go and check out what's saved?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
